{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!gdown https://drive.google.com/uc?id=1ImRFKG1QyNahvUh9en-lwK6NcJTqPUsm\n!tar -zxf /kaggle/working/GAIIC2024-ËµõÈÅì1-ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°.tar.gz\n\n!python /kaggle/working/obj-detection/coco2yolo_train.py\n!python /kaggle/working/obj-detection/coco2yolo_val.py\n\n!python /kaggle/working/obj-detection/jpg2webp.py\n\n!pip install -r /kaggle/working/obj-detection/yolov9/requirements.txt","metadata":{"_uuid":"7bdf1f99-e1cc-474d-969d-812898455562","_cell_guid":"80bb177f-9691-4f00-9b44-2e4d177c7d52","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/obj-detection*\n!git clone https://github.com/LRC-bit/obj-detection.git    #ÊãâÂèñÊúÄÊñ∞ÁöÑobj-detection‰ª£Á†Å","metadata":{"execution":{"iopub.status.busy":"2024-05-03T07:46:39.934930Z","iopub.execute_input":"2024-05-03T07:46:39.935244Z","iopub.status.idle":"2024-05-03T07:46:42.545082Z","shell.execute_reply.started":"2024-05-03T07:46:39.935218Z","shell.execute_reply":"2024-05-03T07:46:42.543862Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'obj-detection'...\nremote: Enumerating objects: 279, done.\u001b[K\nremote: Counting objects: 100% (204/204), done.\u001b[K\nremote: Compressing objects: 100% (105/105), done.\u001b[K\nremote: Total 279 (delta 111), reused 173 (delta 96), pack-reused 75\u001b[K\nReceiving objects: 100% (279/279), 293.52 KiB | 3.16 MiB/s, done.\nResolving deltas: 100% (130/130), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/obj-detection/yolov9/train_dual.py --batch-size=8 --hyp=/kaggle/working/obj-detection/hyp.yaml --data=/kaggle/working/obj-detection/data.yaml --cfg=/kaggle/working/obj-detection/yolov9.yaml","metadata":{"execution":{"iopub.status.busy":"2024-05-03T07:46:44.463899Z","iopub.execute_input":"2024-05-03T07:46:44.464873Z","iopub.status.idle":"2024-05-03T07:49:19.414128Z","shell.execute_reply.started":"2024-05-03T07:46:44.464837Z","shell.execute_reply":"2024-05-03T07:49:19.412856Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2024-05-03 07:46:57.157205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 07:46:57.157305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 07:46:57.323682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=, cfg=/kaggle/working/obj-detection/yolov9/models/detect/yolov9.yaml, data=/kaggle/working/obj-detection/data.yaml, hyp=/kaggle/working/obj-detection/yolov9/hyp.yaml, epochs=100, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=obj-detection/yolov9/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\nYOLO üöÄ 2024-5-3 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0, hsv_s=0, hsv_v=0, degrees=0.0, translate=0.1, scale=0.3, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO üöÄ in ClearML\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO üöÄ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir obj-detection/yolov9/runs/train', view at http://localhost:6006/\nDownloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 13.9MB/s]\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1         0  models.common.Silence                   []                            \n  1                -1  1      2432  models.common.Conv                      [4, 64, 3, 2]                 \n  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n  4                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n  6                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n  8                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n 17                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n 20                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n 26                 0  1      2432  models.common.Conv                      [4, 64, 3, 2]                 \n 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n 29                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n 32                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n 35                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n 38[31, 34, 37, 16, 19, 22]  1  21552062  models.yolo.DualDDetect                 [5, [512, 512, 512, 256, 512, 512]]\nyolov9 summary: 930 layers, 60807614 parameters, 60807582 gradients\n\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 230 weight(decay=0.0), 247 weight(decay=0.0005), 245 bias\nWARNING ‚ö†Ô∏è DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 17957 images, 33 backgrou\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1467 images, 2 backgrounds, 0\u001b[0m\nPlotting labels to obj-detection/yolov9/runs/train/exp/labels.jpg... \n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mobj-detection/yolov9/runs/train/exp\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       0/99      5.34G      7.029      5.877        5.3        171        640:  WARNING ‚ö†Ô∏è TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n       0/99      8.39G      6.879      5.711       5.26        170        640:  \nTraceback (most recent call last):\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 644, in <module>\n    main(opt)\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 538, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 288, in train\n    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1182, in __iter__\n    for obj in iterable:\n  File \"/kaggle/working/obj-detection/yolov9/utils/dataloaders.py\", line 170, in __iter__\n    yield next(self.iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n    return self._process_data(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n    data.reraise()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 694, in reraise\n    raise exception\nIndexError: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/kaggle/working/obj-detection/yolov9/utils/dataloaders.py\", line 695, in __getitem__\n    a_img, labels = self.albumentations(a_img, labels)\n  File \"/kaggle/working/obj-detection/yolov9/utils/augmentations.py\", line 45, in __call__\n    new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\nTraceback (most recent call last):\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 644, in <module>\n    main(opt)\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 538, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/kaggle/working/obj-detection/yolov9/train_dual.py\", line 288, in train\n    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1182, in __iter__\n    for obj in iterable:\n  File \"/kaggle/working/obj-detection/yolov9/utils/dataloaders.py\", line 170, in __iter__\n    yield next(self.iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n    return self._process_data(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n    data.reraise()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 694, in reraise\n    raise exception\nIndexError: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/kaggle/working/obj-detection/yolov9/utils/dataloaders.py\", line 695, in __getitem__\n    a_img, labels = self.albumentations(a_img, labels)\n  File \"/kaggle/working/obj-detection/yolov9/utils/augmentations.py\", line 45, in __call__\n    new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/obj-detection/yolov9/runs*    #Âà†Èô§Êñá‰ª∂Êó∂‰ΩøÁî®","metadata":{"execution":{"iopub.status.busy":"2024-05-02T15:10:46.198395Z","iopub.execute_input":"2024-05-02T15:10:46.198716Z","iopub.status.idle":"2024-05-02T15:10:47.204641Z","shell.execute_reply.started":"2024-05-02T15:10:46.198684Z","shell.execute_reply":"2024-05-02T15:10:47.203161Z"},"trusted":true},"execution_count":39,"outputs":[]}]}